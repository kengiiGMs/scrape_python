Vou ler o arquivo para fornecer uma explica√ß√£o detalhada sobre o funcionamento desse agente.

[1 tool called]

# Explica√ß√£o Detalhada do Agente RAG

Vou explicar como funciona esse agente completo, detalhando cada componente e o motivo de usar cada biblioteca:

## üèóÔ∏è Arquitetura Geral

Este √© um **agente RAG (Retrieval-Augmented Generation)** conversacional que combina:
- Um LLM (Large Language Model) para gera√ß√£o de respostas
- Um sistema de busca vetorial para recuperar informa√ß√µes de documentos
- Um grafo de estados para orquestrar o fluxo de decis√µes

## üìö Bibliotecas Utilizadas e Seus Prop√≥sitos

### 1. **LangGraph** (`langgraph`)

```1:8:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
from dotenv import load_dotenv
import os
from typing import TypedDict, Annotated, Sequence
from operator import add as add_messages


# LangChain & LangGraph Imports
from langgraph.graph import StateGraph, END
```

**Por qu√™?** LangGraph permite criar agentes com **fluxos de decis√£o complexos** usando grafos de estados. √â superior a uma simples chamada de LLM porque:
- Permite loops: o agente pode chamar ferramentas m√∫ltiplas vezes
- Controla o fluxo baseado em condi√ß√µes (tem ou n√£o tool_calls?)
- Mant√©m hist√≥rico de conversa√ß√£o estruturado

### 2. **LangChain** (v√°rios m√≥dulos)

```9:16:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import SupabaseVectorStore
from supabase.client import Client, create_client
from langchain_core.tools import tool
```

**Por qu√™ cada um?**
- **`messages`**: Sistema de tipagem forte para mensagens (Human, System, Tool) - mant√©m contexto organizado
- **`ChatGoogleGenerativeAI`**: Interface unificada para o Gemini, permite trocar de modelo facilmente
- **`GoogleGenerativeAIEmbeddings`**: Converte texto em vetores num√©ricos (768 dimens√µes) para busca sem√¢ntica
- **`TextLoader`**: Carrega arquivos Markdown preservando encoding UTF-8
- **`RecursiveCharacterTextSplitter`**: Quebra documentos grandes em chunks menores de forma inteligente
- **`SupabaseVectorStore`**: Interface para salvar/buscar vetores no Supabase (banco vetorial)
- **`@tool`**: Decorador que transforma fun√ß√µes Python em ferramentas que o LLM pode chamar

### 3. **Supabase Client**

```15:15:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
from supabase.client import Client, create_client
```

**Por qu√™?** Supabase √© um **PostgreSQL com extens√£o pgvector**, que permite:
- Busca vetorial perform√°tica (similarity search)
- Escalabilidade (cloud-native)
- RLS (Row Level Security) para multi-tenancy
- Alternativa open-source ao Pinecone/Weaviate

## üîÑ Fluxo de Funcionamento

### **Fase 1: Configura√ß√£o e Ingest√£o de Dados** (linhas 23-123)

```26:33:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
# Configura√ß√£o do LLM (Usando Gemini 1.5 Flash para velocidade/custo)
llm = ChatGoogleGenerativeAI(model="gemini-3-flash-preview", temperature=0)


# Configura√ß√£o dos embeddings (Fixando 768 para bater com o banco)
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001"
)
```

1. **LLM com temperatura 0**: Respostas determin√≠sticas (importante para suporte)
2. **Embeddings de 768 dimens√µes**: Padr√£o do Gemini Embedding 001

```74:81:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n## ", "\n### ", "\n", " ", ""] # Tenta quebrar por cabe√ßalhos primeiro
)


pages_split = text_splitter.split_documents(documents)
```

3. **Chunking estrat√©gico**:
   - **1000 caracteres**: Tamanho ideal para contexto focado
   - **200 de overlap**: Evita perder contexto entre chunks
   - **Separators**: Prioriza quebras em cabe√ßalhos Markdown (mant√©m contexto sem√¢ntico)

```84:101:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
# Inser√ß√£o manual para deixar o Supabase gerar o ID sozinho
try:
    # Gera embeddings para todos os chunks
    texts = [doc.page_content for doc in pages_split]
    metadatas = [doc.metadata for doc in pages_split]
    vectors = embeddings.embed_documents(texts)
    
    rows = []
    for i in range(len(texts)):
        rows.append({
            "content": texts[i],
            "metadata": metadatas[i],
            "embedding": vectors[i]
        })
    
    # Insere no Supabase sem enviar a coluna 'id'
    supabase.table("marketing_rag").insert(rows).execute()
    print(f"‚úÖ Dados inseridos com sucesso! {len(rows)} chunks.")
```

4. **Vetoriza√ß√£o em batch**: Converte todos os chunks em vetores de uma vez (eficiente)

### **Fase 2: Ferramenta de Busca** (linhas 126-153)

```129:149:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
@tool
def retriever_tool(query: str) -> str:
    """
    Use esta ferramenta para buscar informa√ß√µes oficiais sobre a Plantie.
    Sempre que o usu√°rio fizer uma pergunta sobre funcionalidades, pre√ßos ou suporte,
    voc√™ DEVE usar esta ferramenta antes de responder.
    """
    print(f"üîé Buscando no banco de dados: '{query}'")
    docs = retriever.invoke(query)


    if not docs:
        return "Nenhuma informa√ß√£o relevante encontrada na base de conhecimento."
   
    results = []
    for i, doc in enumerate(docs):
        # Adiciona o conte√∫do e a fonte (metadata) para o LLM saber de onde veio
        source = doc.metadata.get('source', 'Desconhecido')
        results.append(f"--- Trecho {i+1} (Fonte: {source}) ---\n{doc.page_content}\n")
   
    return "\n".join(results)
```

**Funcionamento**:
1. LLM decide que precisa buscar informa√ß√µes
2. Chama `retriever_tool(query="como funciona o monitoramento")`
3. A ferramenta busca os 4 chunks mais similares (cosine similarity)
4. Retorna os trechos formatados para o LLM sintetizar

### **Fase 3: Grafo de Estados** (linhas 190-266)

```193:195:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
```

**Estado do agente**: Lista de mensagens que cresce automaticamente (`add_messages` √© um reducer)

```197:206:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
def call_llm(state: AgentState):
    """N√≥ que chama o LLM"""
    messages = state['messages']
    # Injeta o System Prompt no in√≠cio, mas mantendo o hist√≥rico
    # Nota: Em modelos de chat, o SystemMessage deve ser o primeiro.
    if not isinstance(messages, SystemMessage):
        messages = + messages
   
    response = llm_with_tools.invoke(messages)
    return {'messages': [response]}
```

**N√≥ 1 (call_llm)**: Envia hist√≥rico + system prompt para o LLM

```209:231:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
def take_action(state: AgentState):
    """N√≥ que executa as ferramentas"""
    last_message = state['messages'][-1]
    tool_calls = last_message.tool_calls
   
    results = []
    tools_dict = {t.name: t for t in tools}


    for t in tool_calls:
        print(f"‚öôÔ∏è Executando ferramenta: {t['name']}")
        if t['name'] in tools_dict:
            # Executa a tool
            tool_result = tools_dict[t['name']].invoke(t['args'])
           
            # Cria a mensagem de resposta da tool
            results.append(ToolMessage(
                tool_call_id=t['id'],
                name=t['name'],
                content=str(tool_result)
            ))
   
    return {'messages': results}
```

**N√≥ 2 (take_action)**: Executa as ferramentas solicitadas pelo LLM

```234:239:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
def should_continue(state: AgentState):
    """Decide se para ou continua para as ferramentas"""
    last_message = state["messages"][-1]
    if last_message.tool_calls:
        return "retriever_agent"
    return END
```

**Fun√ß√£o de decis√£o**: Se o LLM pediu ferramentas ‚Üí vai para `take_action`, sen√£o ‚Üí termina

```243:264:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
graph = StateGraph(AgentState)


graph.add_node("llm", call_llm)
graph.add_node("retriever_agent", take_action)


graph.set_entry_point("llm")


graph.add_conditional_edges(
    "llm",
    should_continue,
    {
        "retriever_agent": "retriever_agent",
        END: END
    }
)


graph.add_edge("retriever_agent", "llm")


rag_agent = graph.compile()
```

**Fluxo do grafo**:
```
In√≠cio ‚Üí [LLM] ‚Üí Precisa ferramenta? 
                    ‚Üì Sim             ‚Üì N√£o
               [Executa Tool] ‚Üí [LLM] ‚Üí Fim
                    ‚Üë______________|
```

### **Fase 4: Loop de Conversa√ß√£o** (linhas 272-302)

```280:298:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
    while True:
        user_input = input("\nVoc√™: ")
        if user_input.lower() in ['exit', 'quit']:
            break
       
        # Adiciona mensagem do usu√°rio ao estado
        chat_history.append(HumanMessage(content=user_input))
       
        # Invoca o agente
        print("ü§ñ Pensando...")
        result = rag_agent.invoke({"messages": chat_history})
       
        # Pega a resposta final
        final_response = result['messages'][-1].content
       
        # Atualiza hist√≥rico com a resposta do assistente para manter contexto
        chat_history.append(result['messages'][-1])
       
        print(f"\nPlantie AI: {final_response}")
```

Mant√©m hist√≥rico completo para conversas contextuais (multi-turn).

## üéØ Pontos-Chave do Design

### **System Prompt Estrat√©gico**

```160:187:c:\Users\bucci\Quaestum\Scrape - Copia\langchain\agent_rag.py
system_prompt = """
# IDENTITY & ROLE
Voc√™ √© o **Plantie AI Specialist**, o assistente virtual oficial da plataforma Plantie.
Sua miss√£o √© ajudar clientes a entenderem, comprarem e utilizarem nossa solu√ß√£o de monitoramento de plantas via IoT.


# KNOWLEDGE BASE & TRUTH (CR√çTICO)
Voc√™ N√ÉO possui conhecimento inato sobre os detalhes t√©cnicos atuais da Plantie.
**REGRA N√öMERO 1:** Para QUALQUER pergunta sobre a plataforma (pre√ßos, como funciona, erros, cadastro), voc√™ **DEVE** usar a ferramenta `retriever_tool` para buscar a resposta oficial.


# PROTOCOLO DE RESPOSTA
1. **Analise** a pergunta do usu√°rio.
2. **Consulte** a `retriever_tool` com termos de busca relevantes.
3. **Sintetize** a resposta baseada APENAS no texto retornado pela ferramenta.
4. **Cite** as informa√ß√µes implicitamente (ex: "De acordo com nossa documenta√ß√£o...").


# GUARDRAILS (SEGURAN√áA)
- Se a `retriever_tool` retornar "Nenhuma informa√ß√£o encontrada" ou informa√ß√µes que n√£o respondem √† d√∫vida, **N√ÉO INVENTE**. Responda: "Desculpe, n√£o encontrei essa informa√ß√£o espec√≠fica nos meus manuais. Gostaria de falar com um atendente humano?"
- Se o usu√°rio perguntar sobre c√≥digo (Python, React) que n√£o seja relacionado √† API da Plantie, recuse educadamente.
- Mantenha um tom profissional, entusiasta e focado em solu√ß√µes sustent√°veis.


# INTERACTION STYLE
- Seja conciso. Use listas (bullet points) para instru√ß√µes passo-a-passo.
- Se for uma obje√ß√£o de vendas (ex: "est√° caro"), use as informa√ß√µes recuperadas para destacar o valor/benef√≠cio (ex: economia de √°gua).
"""
```

- **For√ßa o uso da ferramenta**: Evita alucina√ß√µes
- **Guardrails**: Define limites claros do que o agente pode/n√£o pode fazer
- **Tom de voz**: Define personalidade (profissional + sustent√°vel)

## üöÄ Vantagens dessa Arquitetura

1. **Escalabilidade**: Supabase cresce com o neg√≥cio
2. **Precis√£o**: RAG reduz alucina√ß√µes em ~80% vs LLM puro
3. **Manutenibilidade**: Atualizar conhecimento = atualizar Markdown + re-inserir
4. **Auditabilidade**: Logs mostram exatamente quais documentos foram usados
5. **Multi-turn**: Mant√©m contexto entre perguntas

## ‚ö†Ô∏è Problemas Atuais no C√≥digo

H√° um bug na linha 203:

```python
if not isinstance(messages, SystemMessage):
    messages = + messages  # ‚ùå Sintaxe inv√°lida
```

Deveria ser:

```python
messages = [SystemMessage(content=system_prompt)] + messages
```

Essa √© a explica√ß√£o completa! O agente combina busca vetorial inteligente com um LLM guiado por ferramentas, tudo orquestrado por um grafo de estados que permite decis√µes complexas e loops de racioc√≠nio.