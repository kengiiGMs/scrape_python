from bs4 import BeautifulSoup
from urllib.parse import urljoin

from scrapers.scrape_playwright import iniciar_playwright
from scrapers.scrape_request import iniciar_request

def processar_scrape_completo(url):
    print("ğŸ” Tentando com Requests ...")
    status, html = iniciar_request(url)

    if not status or html is None:
        print("âš ï¸ Falha no Request , usando Playwright...")
        status, html = iniciar_playwright(url)
        if not status or html is None:
            print("âš ï¸ Falha playwright")
            return False, None

    print("âœ… Sucesso com raspagem!")

    # Adiciona a pÃ¡gina principal como a primeira da lista
    paginas = [{
        'link': {'texto': 'PÃ¡gina Principal', 'url': url},
        'html': html,
        'status': True
    }]

    print("ğŸ”„ï¸ Capturando links das pÃ¡ginas")
    html_formatado = BeautifulSoup(html, 'html.parser')

    links_http = []
    urls_vistas = {url}
    if url.endswith('/'):
        urls_vistas.add(url[:-1])
    else:
        urls_vistas.add(url + '/')

    termos_ignorados = ['facebook', 'whatsapp', 'instagram', 'youtube', 'twitter', 'x.com', 'pinterest', 'wa.me', 'linkedin']

    header = html_formatado.find('header')
    footer = html_formatado.find('footer')
    links_a = []
    if header:
        links_header = header.find_all('a', href=True)
        if links_header:
            print("ğŸ”„ï¸ Capturando links no header")
            links_a = links_header
    if not links_a and footer:
        links_footer = footer.find_all('a', href=True)
        if links_footer:
            print("ğŸ”„ï¸ Capturando links no footer")
            links_a = links_footer
    if not links_a:
        print("ğŸ”„ï¸ Erro no header e no footer ou nenhum link encontrado, Capturando links da pagina toda!")
        links_a = html_formatado.find_all('a', href=True)

    for link in links_a:
        if len(links_http) >= 20:
            break
        href = link['href']
        # Ignora links que contenham '#'
        if '#' in href:
            continue
        url_completa = urljoin(url, href)
        # Filtra apenas URLs que comeÃ§am com http ou https
        if url_completa.startswith(('http://', 'https://')):
            url_lower = url_completa.lower()
            if any(termo in url_lower for termo in termos_ignorados):
                continue
            if url_completa not in urls_vistas:
                urls_vistas.add(url_completa)
                links_http.append({
                    'texto': link.get_text().strip(),
                    'url': url_completa
                })
                print(f"{link.get_text()}: {url_completa}")

    print('\nğŸ”„ï¸ Iniciando Scrape dos links das pÃ¡ginas coletadas\n')

    for link in links_http:
        print(f"Iniciando scrape do link {link['texto']}")
        print("ğŸ” Tentando com Requests ...")
        status, html = iniciar_request(link['url'])

        if not status or html is None:
            print("âš ï¸ Falha no Request , usando Playwright...")
            status, html = iniciar_playwright(link['url'])
            if not status or html is None:
                print("âš ï¸ Falha playwright")
                paginas.append({'link': link, 'html': None, 'status': False})
                continue

        print("âœ… Sucesso com raspagem!")

        paginas.append({
            'link': link,
            'html': html,
            'status': True
        })

        print(f"âš ï¸ Finalizando procedimento de scrape para o link: {link['texto']}\n")

    return True, paginas